---
title: "model_analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# load packages and data
```{r}
library(ggplot2) #for plotting
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(tidymodels) #for modeling

#path to data
#note the use of the here() package and not absolute paths
data_location <- here::here("data","processed_data","processeddata.rds")

#load cleaned data. 
mydata <- readRDS(data_location)
```

# split data into train and test subsets
```{r}
# set seed for reproducible analysis (instead of random subset each time)
set.seed(222)
#subset 3/4 of data as training set
data_split <- initial_split(mydata, prop = 3/4)

#save sets as data frames
train_data <- training(data_split)
test_data <- testing(data_split)
```

# recipe to fit categorical outcome to all predictors
```{r}
# categorical outcome: Nausea

#recipe for categorical outcome with all predictors
nausea_rec <- 
  recipe(Nausea ~ ., data = train_data)

#logistic model set up
log_mod <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

#workflow set up
nausea_wflow <- 
  workflow() %>% add_model(log_mod) %>% add_recipe(nausea_rec)

#use workflow to prepare recipe and train model with predictors
nausea_fit <- 
  nausea_wflow %>% fit(data = train_data)

#extract model coefficient
nausea_fit %>% extract_fit_parsnip() %>% tidy()
```

# evaluate model
```{r}
#look at ROC and ROC-AUC (performance metric for categorical outcomes)
#used trained workflow to predict using test data
predict(nausea_fit, test_data)

#include probabilities
nausea_aug <- augment(nausea_fit, test_data)

#generate ROC curve
nausea_aug %>%
  roc_curve(truth = Nausea, .pred_No) %>% autoplot()

#estimate area under the curve
nausea_aug %>% roc_auc(truth = Nausea, .pred_No)

```

We have an ROC-AUC of 0.724, indicating that the model could be useful! And, the ROC looks similar to the Tidymodels tutorial

# fit alternative model using only main predictor
```{r}
#categorical outcome: Nausea
#main predictor: RunnyNose

#recipe for categorical outcome with Runny Nose predictor
runnynose_rec <- 
  recipe(Nausea ~ RunnyNose, data = train_data)

#logistic model set up
log_mod <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

#workflow set up
runnynose_wflow <- 
  workflow() %>% add_model(log_mod) %>% add_recipe(runnynose_rec)

#use workflow to prepare recipe and train model with predictors
runnynose_fit <- 
  runnynose_wflow %>% fit(data = train_data)

#extract model coefficient
runnynose_fit %>% extract_fit_parsnip() %>% tidy()
```

# evaluate alternative model with only main predictor
```{r}
#look at ROC and ROC-AUC (performance metric for categorical outcomes)
#used trained workflow to predict using test data
predict(runnynose_fit, test_data)

#include probabilities
runnynose_aug <- augment(runnynose_fit, test_data)

#generate ROC curve
runnynose_aug %>%
  roc_curve(truth = Nausea, .pred_No) %>% autoplot()

#estimate area under the curve
runnynose_aug %>% roc_auc(truth = Nausea, .pred_No)
```

For the alternative model, we have an ROC-AUC of 0.465, which tells us this model is no good for predictions



\newpage
# Part 2: 
## recipe to fit continious outcome to all predictors
```{r}
# continuous outcome: BodyTemp

#recipe for continuous outcome with all predictors
BT_rec <- recipe(BodyTemp ~ ., data = train_data)

#linear model set up
lm_model <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')

#workflow set up
BT_wflow <- 
  workflow() %>% add_model(lm_model) %>% add_recipe(BT_rec)

#use workflow to prepare recipe and train model with predictors
BT_fit <- 
  BT_wflow %>% fit(data = train_data)

#extract model coefficient
BT_fit %>% extract_fit_parsnip() %>% tidy()
```

# evaluate model
```{r}
#look at RMSE (performance metric for continuous outcomes)
#used trained workflow to predict using test data
predict(BT_fit, test_data)

#include probabilities
BT_aug <- augment(BT_fit, test_data)

#generate RMSE, setting the "truth" and .pred values
BT_aug %>%
  rmse(truth = BodyTemp, .pred)

# Plotting results
ggplot(BT_aug, aes(x = BodyTemp, y = .pred)) + 
  # Create a diagonal line:
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.5) + 
  labs(y = "Predicted Body Temp (f)", x = "Body Temp (f)") +
  # Scale and size the x- and y-axis uniformly:
  coord_obs_pred()

# r-squared is another good metric
metrics <- metric_set(rmse, rsq, mae)
metrics(BT_aug, truth = BodyTemp, estimate = .pred)

```

We have an RMSE of 1.148515. Not too bad, the lower the better for RMSE. 
Not too surprised about the distribution of body temp, considering a majority of participants had normal body temps (around 98 degrees)
r-squared is 0.0468187	

# fit alternative model using only main predictor
```{r}
#continuous outcome: BodyTemp
#main predictor: RunnyNose

#recipe for continuous outcome with all predictors
BT_rec2 <- recipe(BodyTemp ~ RunnyNose, data = train_data)

#linear model set up
lm_model2 <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')

#workflow set up
BT_wflow2 <- 
  workflow() %>% add_model(lm_model2) %>% add_recipe(BT_rec2)

#use workflow to prepare recipe and train model with predictors
BT_fit2 <- 
  BT_wflow2 %>% fit(data = train_data)

#extract model coefficient
BT_fit2 %>% extract_fit_parsnip() %>% tidy()
```

# evaluate alternative model with only main predictor
```{r}
#look at RMSE (performance metric for continuous outcomes)
#used trained workflow to predict using test data
predict(BT_fit2, test_data)

#include probabilities
BT_aug2 <- augment(BT_fit2, test_data)

#generate RMSE, setting the "truth" and .pred values
BT_aug2 %>%
  rmse(truth = BodyTemp, .pred)

# Plotting results
ggplot(BT_aug2, aes(x = BodyTemp, y = .pred)) + 
  # Create a diagonal line:
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.5) + 
  labs(y = "Predicted Body Temp (f)", x = "Body Temp (f)") +
  # Scale and size the x- and y-axis uniformly:
  coord_obs_pred()

# r-squared is another good metric
metrics <- metric_set(rmse, rsq, mae)
metrics(BT_aug2, truth = BodyTemp, estimate = .pred)

```
We have an RMSE of 1.134553. Lower than the first model. 
Once again the distribution of body temp is looking a bit odd
r-squared is 0.02399179	

